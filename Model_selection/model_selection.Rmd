---
title: "Model selection"
output: html_document
date: "2025-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(tidyverse)
library(ggplot2)
# for plotting PCA
library(ggfortify) 
# for negative binomial models
library(MASS)
library(MuMIn)
# To make predictions
library(ggeffects)
# for the backward model selection function called regsubsets
library(leaps)

```

```{r}
bees <- read.csv("Eulaema.csv")
head(bees)
```


```{r}
bees <- tibble(bees)
head(bees)
```


# PCA

One hot encode non numeric columns in the data set.
```{r}
# check classes of all columns
lapply(bees, class) 

# Check if the columns have any type of grouping my checking how many unique values they have
length(bees$SA)
length(unique(bees$SA))
length(unique(bees$method))
length(unique(bees$SU))
```

```{r}
# SA have grouping (72) and method (3), but SU does not

# One-hot encode SA, meaning put it into 72 columns with 1s and 0s
bees <- bees %>% mutate(value = 1)  %>% spread(SA, value,  fill = 0 )

# Make the mehtod columns into numbers
head(bees$method)
bees$method <- as.integer(factor(bees$method))
head(bees$method)

# Remove SU column
bees <- bees |> select(-SU)

# Check the results
head(bees)

```

In the method column now, 1 = Net, 2 = NetTraps and 3 = Traps.

Remove non utf8 characters from the column names


```{r}
# Strip all non-UTF-8 characters
names(bees) <- iconv(names(bees), from = "", to = "UTF-8", sub = "")
```


```{r}
pca_bee <- prcomp(bees, scale. = TRUE, center = TRUE, retx = TRUE)
names(pca_bee)
summary(pca_bee)
# Show the weights/loadings/coefficients
head(pca_bee$rotation)

```
```{r}
# npcs is nr of principal components to plot, deafult is 10
screeplot(pca_bee, type = "lines", npcs = 20)
```
default variances is pca_bee$sdev^2\
THe plot shows the variance, I want to see the proportion of variances explained by each PC
```{r}
var_explained <- pca_bee$sdev^2
prop_var <- var_explained / sum(var_explained)

# keep only the first 20 PCs
prop_var_20 <- prop_var[1:20]

plot(prop_var_20,
     type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "Scree Plot")


```
SO the highest proportion of variance explained is 4%. Which more or less means that the variables are not very correlated. One needs many principal components to find some linearity in the data. I will continue with trying to remove the SA columns, which are 72 columns on its on. Just to see if that makes a difference to the number of principal components needed. \
\

The plot looks exacly the same so the SA columns probably dosent have so much influence.\

Visualize PCA
```{r}
biplot(pca_bee, main = "Biplot of Principal Components", scale = 0)
```
Compute standard deviation and variance
```{r}
pca_bee.var <- pca_bee$sdev^2

cat("Standard Deviation :",pca_bee$sdev,"\n")
cat("Variance :",pca_bee.var,"\n")
```

Make elbow plot
```{r}

# Variance of each PC
var_explained <- pca_bee$sdev^2

# Scree (elbow) plot
plot(var_explained,
     type = "b",
     xlab = "Principal Component",
     ylab = "Variance Explained",
     main = "Scree Plot")

```

Remove SA columns\
I could re-load the data and redo all the steps. But i think for cleanness, reproducibility for it to be easier to follow the flow of the script, I will use the tibble I created and remove the last 72 columns, which are the SA columns that I added earlier in the script.
```{r}
bees <- bees |> select(1:10)
head(bees)
```

```{r}
# Make a new PCA with the shorter tibble
short_PCA <- prcomp(bees, scale. = TRUE, center = TRUE, retx = TRUE)

# Make scree plot with proportion of variance explained
var_explained <- short_PCA$sdev^2
short_prop_var <- var_explained / sum(var_explained)

plot(short_prop_var,
     type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "Scree Plot")

```

visualize the pca
```{r}

# Change method column to facors so it can be plotted as three different things instead of a contious scale


bees <- bees |> mutate(method = as_factor(method))

autoplot(short_PCA,
         data = bees,
         colour = 'method')

autoplot(short_PCA,
         data = bees,
         colour = 'effort')

autoplot(short_PCA,
         data = bees,
         colour = 'altitude')

autoplot(short_PCA,
         data = bees,
         colour = 'MAT')

autoplot(short_PCA,
         data = bees,
         colour = 'MAP')

autoplot(short_PCA,
         data = bees,
         colour = 'Tseason')

autoplot(short_PCA,
         data = bees,
         colour = 'Pseason')

autoplot(short_PCA,
         data = bees,
         colour = 'forest.')

autoplot(short_PCA,
         data = bees,
         colour = 'lu_het')


```

I can see clear clustering on method, MAT
However some of the other variables could be affected by the fact that they have some extreme values that shifts the color scale quite a lot. 
Anyways I will do the model selection analysis looking at method and MAT

# Model selection with AIC
Check for over dispersion.\
Poisson assumes that the variance = mean
```{r}
mean(bees$Eulaema_nigrita)
var(bees$Eulaema_nigrita)
```
Looks like overdisperion so I will do the negative binomial model\
\
Make the models\
+ adds a main effect.\
* adds both main effects and their interaction\
Meaning:\
A * B\
Expands to: A + B + A:B\
That is:\
the effect of A\
the effect of B\
the interaction between A and B
```{r}
m1 = glm.nb(Eulaema_nigrita~effort+altitude+MAT, data=bees)
m2 = glm.nb(Eulaema_nigrita~effort+altitude, data=bees)
m3 = glm.nb(Eulaema_nigrita~effort+MAT, data=bees)
m4 = glm.nb(Eulaema_nigrita~effort+altitude * method, data=bees)
m5 = glm.nb(Eulaema_nigrita~effort+MAT * method, data=bees)
m6 = glm.nb(Eulaema_nigrita~effort+altitude+MAT+method+MAP+Tseason+Pseason+forest.+lu_het, data=bees)
m7 = glm.nb(Eulaema_nigrita~effort+altitude+MAT * method, data=bees)
m8 = glm.nb(Eulaema_nigrita~effort+altitude+MAP+Tseason+Pseason+forest.+lu_het*MAT*method, data=bees)
m9 = glm.nb(Eulaema_nigrita~effort+altitude+MAP+Tseason+Pseason+forest.+lu_het+MAT*method, data=bees)
m10 = glm.nb(Eulaema_nigrita~effort+altitude+MAP+Tseason+Pseason+forest.+lu_het+method*MAT, data=bees)

```

When comparing models one can use information criteria. One of this type is AIC, which is defined as AIC = -2ln(L_hat) + 2k, where ln(L_hat) is the log likelihood of the model and k is the number of free parameters in the model. The likelihood represents the probability of the data given some parameters. The lower the AIC the better the model. Which means that the models are penalized for having many parameters (with the +2k) so that simpler models are preferd.\
A general rule of thumb is that 2 AIC units indicates strong support.\
Calculate the AIC for all models and compare them.

```{r}
 mlist = list(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10)
 AICTab = AIC(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10)
 AICTab = AICTab[order(AICTab$AIC, decreasing=F),]
 AICTab

```

The model where all variables are all included, and without interaction effects is the best one. \
\
\
# Backward selection
I will try to use a backward selection method I used before to see if i get to the same conclusion\
the function used here is called regsubsets, it uses RSS to fit the best model as default 
```{r}
backward_m <- regsubsets(
  Eulaema_nigrita~.,
  data=bees,
  nvmax= 35,
  method = "backward"
  )

# Summary the model output
model_summary <- summary(backward_m)
model_summary

```

```{r}
# Look at the RSS of the model 
model_summary$rss

# Plot number of predictors against the RSS to see where the model has the best fit without being unnecessary large
plot(model_summary$rss, type = "b", xlab = "Number of predictors", ylab = "RSS")



```

From the plot above 8 variables seems to be the best model.
```{r}
# Extract the coefficients of the "best" model
# Named numeric vector
coef_8 <- coef(backward_m, 8)
coef_8

```
This means that if you use backward selection and RSS as a measurement the best model seems to be when all variables except lu_het are included.

```{r}
# Define the "best" formula, based on the 8 predictors
# Get variable names (excluding intercept)
best_vars <- names(coef_8)[-1]
best_vars

# Construct a formula automatically
formula_best <- as.formula(
  paste("Eulaema_nigrita~", paste(best_vars, collapse = " + "))
)

formula_best
```